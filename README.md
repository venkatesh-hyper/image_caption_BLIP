# BLIP_image_caption_model 

its a basic working of already pretrained model which is present in <b>hugging face</b> 
  url of the model:- https://huggingface.co/Salesforce/blip-image-captioning-base/tree/main
  
# BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation
    Model card for image captioning pretrained on COCO dataset - base architecture (with ViT base backbone).

  # we develop an ui for using the model in local machine by using gradio

    <img width="1470" alt="Screenshot 2024-09-16 at 7 35 04â€¯AM" src="https://github.com/user-attachments/assets/8d964025-2c38-47b6-825b-57090d294a3a">
